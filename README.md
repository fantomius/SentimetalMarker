# SentimetalMarker
Приложение, различающее эмоциональную окраску предложений<br/>
<br/>
# Зависимости
Программа написана на Python 3 и зависит от следующих компонент:<br/>
numpy, scipy и PyQt4 (рекомендуется ставить дистрибутив Anaconda отсюда - https://www.continuum.io/downloads).<br/>
<br/>
Для дальнейшей установки достаточно запустить setup.py.<br/>
<br/>
Для информации, программа так же зависит от следующих библиотек:<br/>
-pymorphy2<br/>
-pymorphy2-dicts-ru<br/>
-nltk<br/>
<br/>
Для nltk через nltk.download() надо поставить следующие пакеты:<br/>
-stopwords<br/>
-punkt<br/>
-averaged_perceptron_tagger<br/>
-averaged_perceptron_tagger_ru<br/>
<br/>
# Запуск
Запускать main.py
# Решение 
Просмотрев несколько источников, я нашел два возможных подхода к этой задаче:<br/>
1) Разбиение предложения на слова, оценки слов по отдельности и выставление итоговой оценки на их основе <br/>
2) Обучение по корпусу размеченных текстов и определение близости переданного предложения к тому или иному эталону<br/>
<br/>
Так как никакого размеченного корпуса предложений я не нашел, то было принятно решение использовать 1-ый подход. <br/>
Определять окраску слова можно так же двумя способами: <br/>
1) Можно оценивать априорную вероятность, что предложение с таким словом является, например, положительным, на основе апостериорной вероятности, оцененной по корпусу (вероятность, что такое слово встречается в положительном предложении)<br/>
2) Можно использовать имеющиеся размеченные словари<br/>
<br/>
Я остановился на 2-м подходе. Возможно, стоит добавить ещё и 1-ый, но он имеет ряд "НО": <br/>
а) Нужен достаточно большой корпус <br/>
б) Оценка вероятностей сильно зависит от корпуса. Соответственно, если итоговые предложения будут отличаться от тех, на которых мы учились, то будут получены неверные результаты<br/>
<br/>
После того, как я принял решение использовать 2-ой подход, я занялся поиском словарей. К сожалению, для русского языка всё не так радужно, как для английского. Я нашел 3 разных словаря с разными шкалами оценок: <br/>
1) https://github.com/Wobot/Sentimental <br/>
2) http://lilu.fcim.utm.md/resourcesRoRuWNA.html <br/>
3) http://linis-crowd.org/ <br/>
<br/>
В 3-м варианте был ещё и корпус размеченных текстов.<br/>
<br/>
Так как эти словари не очень большие, то я принял решение использовать их все. Дело оставалось за малым - построить готовое решение.<br/>
<br/>
Изначально была сделана загрузка этих словарей + возвращение оценки для каждого слова по ним (см. dictionaries.py). <br/>
Далее был написан SentimentalEstimator( sentimental_estimator.py ), который разбивает предложение на слова, оценивает их по всем словарям в отдельности и возвращает результат <br/>
Результат определяется по очень простой формуле: если pos / negative < neutral_treshold, то результат - нейтральный, иначе возвращаем pos или negative. <br/>
<br/>
Разбиение предложения на слова происходит так: <br/>
а) Сначала мы удаляем из предложения пунктуацию и цифры <br/>
б) Затем мы разбиваем его на слова <br/>
в) И наконец приводим каждое слово в начальную форму (для существительных это - ед.ч., м.р., и.п.) <br/>

Возникла потребность в оценке полученной машины. Для этого был написан класс TextsCorpus(textscorpus.py) и вспомогательная функция (get_sentences_quality), которые прогоняли алгоритм по базе из linis-crowd и писали количество правильно классифицированных ответов. <br/>
<br/>
Сразу же встал вопрос, как правильно настроить веса каждого словаря и neutral_treshold. Для этого я воспользовался дифф эволюцией (её можно найти в corpus_evolution.py). Я согласен, что оценивать и настраивать машинку на корпусе длинных текстов, а не отдельных предложений - задача немного другая. Впрочем, я думаю, что это всё-таки улучшит результаты. <br/>
<br/>
Ну и наконец был написан некий GUI на qt4, который даёт возможность всё это поприменять и посмотреть, как оно работает. <br/>
<br/>
# Дальнейшие улучшения
а) Собрать корпус предложений <br/>
б) Добавить отнесение к тому или иному классу на основе вероятностного подхода <br/>
в) Соединить это с текущим подходом с оценкой по словарю<br/>
г) Разные функции отнесения к тому или иному классу. Например, сначала отделять нейтральные, а затем уже делить на позитивные и негативные<br/>
